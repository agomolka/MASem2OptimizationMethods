# MASem2OptimizationMethods
Project with non-classical Optimization Methods

In a world where the amount of available data is growing at a staggering pace, data science and machine learning have become pillars of modern science and technology. Predictive models are used in various fields of science and industry to help solve complex problems such as disease diagnosis, stock price forecasting, and image recognition. In each of these cases, the model must be precisely calibrated to achieve the highest possible accuracy and performance. A key element of this process is the selection of hyperparameters. Hyperparameters are values that define the structure of the model and the way it learns. Unlike parameters, which are learned features of the model, hyperparameters are not directly learned from the data but are set a priori. For example, in the case of a Gradient Boosting Classifier (GBC), hyperparameters may include the number of estimators, learning rate, or the depth of the decision tree.
Hyperparameter tuning is an optimization problem that affects a complex space where each choice of hyperparameters can lead to a different model quality. The ultimate goal is to find a set of hyperparameters that minimize the error on validation data while preventing overfitting. One approach to hyperparameter optimization is Grid Search, which involves systematically searching through a grid of hyperparameter values. Although it is a simple and straightforward approach, it can be computationally expensive, especially for large hyperparameter spaces. An alternative is Bayesian Optimization, which is a more advanced optimization method. It involves modeling the probability distribution of the hyperparameter space and utilizes this distribution to intelligently choose the next points for evaluation. In this work, we focus on analyzing these two techniques applied to the problem of hyperparameter optimization in the context of machine learning models. We will analyze their effectiveness, efficiency, and properties in the context of different datasets. In particular, we will investigate the convergence of algorithms, the impact of different hyperparameters on model quality, and the sensitivity of results to changes in data.
The conclusion of this analysis can be valuable not only for practitioners and scientists but can also lead to a deeper understanding of the fundamental properties of hyperparameter optimization algorithms. By approaching the problem holistically, we also aim to contribute to the development of new concepts and strategies that may be useful in future research in this field.
